<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <meta name="description" content="This is an awesome website">
    <title>Nour Makke</title>
    <style>
      body{
        background: #F6F6F6;
        color:midnightblue;
        font-size: 16px;
        margin-left:0.0em;
        margin-right:0.0em;
        margin-top:0.0em;
      }
      .header{
        height: 200px;
        padding: 0.5px;
        text-align: center;
        color: white;
        background: midnightblue;
        color: #F6F6F6;
        font-size: 15px;
        margin:0;
        border-bottom: 3px solid #cc3300;
      }
      .content {
        padding:20px;
        }
      .sidenav {
        height: 100%;
        width: 160px;
        position: absolute;
        z-index: 1;
        top: 6em;
        left: 0;
        background-color: #F6F6F6;
        overflow-x: hidden;
        padding-top: 0px;
        }
      .sidenav a {
        padding: 6px 8px 10px 20px;
        text-decoration: none;
        font-size: 16px;
        font-weight: bold;
        color: midnightblue;
        display: block;
      }
    .sidenav a:hover {
        color: #f1f1f1;
        background-color:#cc3300;
      }
    .vl {
      border-left: 1px solid red;
      height: 500px;
      position: absolute;
      left: 17%;
      margin-left: -3px;
      margin-top: 20px;
      top: 10;
    }
    .header img {
      float: left;
      width: 60px;
      height: 60px;
      background:midnightblue; /*#555;*/
    }
    </style>
  </head>
  <body>
    <div class="header">
      <!--img src="Logo_CERN.png" alt="logo" /-->
<!--       <img src="cern-logo.png" alt="logo"> -->
      <h1 style="color:white; margin-top:2.0em"> Symbolic Regression:<br> A Pathway To Interpretability Towards Automated Scinetific Discovery</h1>
      <h2> Tutorial@KDD2024, Sunday, August 25, 10:00 am - 1:00 pm</h2>
    </div>
        <header>
        <h1>Training Large-scale Foundation Models on Emerging AI Chips</h1>
        <p>Tuesday, 8 August, from 2:00pm to 5:00pm in room 202B</p>
    </header>
    <main>
        <section>
            <h2>Abstract</h2>
            <p>Foundation models such as ChatGPT and GPT-4 have garnered significant interest from both academia and industry due to their emergent capabilities, such as few-shot prompting, multi-step reasoning, instruction following, and model calibration. Such capabilities were previously only attainable with specially designed models, such as those using knowledge graphs, but can now be achieved on a much larger scale with foundation models. As the capabilities of foundation models have increased, so too have their sizes at a rate much faster than Moore's Law. For example, the BERT large model, released in 2018, was a 324M-parameter model. The Pathways Language Model (PaLM), released in 2022, was trained with 540B parameter, which represents an increase of more than three orders of magnitude in just 4 years. The training of foundation models requires massive computing power. For instance, training a BERT model on a single state-of-the-art GPU machine with multi-A100 chips can take several days, while training GPT-3 models on a large multi-instance GPU cluster can take several months to complete the estimated 3*10^23 flops.</p>
            <p>This tutorial provides an overview of the latest progress in supporting foundation model training and inference with new AI chips. It reviews progress on the modeling side, with an emphasis on the transformer architecture, and presents the system architecture supporting training and serving foundation models. This includes programming language frameworks such as PyTorch and Tensor Flow, graph compilers, 3D parallelism, and accelerators such as the GPU, H100, TPU, and Trainium. Finally, the tutorial presents our experience of training foundation models using different systems.</p>
        </section>
        <section>
            <h2>Slides</h2>
            <p>You can download the slides <a href="#">here</a>.</p>
        </section>
    </main>
<!--     <hr style="margin-top:0.1em;height:1px;border-width:0;color:gray;background-color:#cc3300">
<!--     <div class="sidenav">
      <tr><a href="">Home</a></tr></tr>
      <tr><a href="index_publications.html">Reasearch & Publications</a></tr>
      <tr><a href=".html">Talks & Seminars</a></tr>
      <tr><a href=".html">Activities</a></tr>
      <tr><a href="index_teaching.html">Teaching</a></tr>
      <tr><a href="http://home.cern">CERN</a></tr>
      <tr><a href="index_technical.html">Technical</a></tr>
      <tr><a href=".html">Contact</a></tr>
    </div> -->
    <div class="vl"> </div> -->
    <div style="margin-left:15.0em; margin-right:10em">
      <h1 style="text-align: center;"> Abstract</h1>
      <p style="text-align: center;">
          Symbolic regression is a machine learning technique employed for learning mathematical equations directly from data. Mathematical equations capture both functional and causal relationships in the data. In addition, they are simple, compact, generalizable, and interpretable models, making them the best candidates for i) learning inherently transparent models and ii) boosting scientific discovery. Symbolic regression has received a growing interest since the last decade and is tackled using different approaches in supervised and unsupervised deep learning, thanks to the enormous progress achieved in deep learning in the last twenty years. 
  Symbolic regression remains underestimated in conference coverage as a primary form of interpretable AI and a potential candidate for automating scientific discovery. This tutorial overviews symbolic regression: problem definition, approaches, and key limitations, discusses why physical sciences are beneficial to symbolic regression, and explores possible future directions in this research area.
      </p>
      <!--p><b> Physics 104: Classical Mechanics, Fluid dynamcis and Thermodynamics (undergraduate)</b><p-->
      <p>&nbsp;</p>
<!--       <p> I am Nour Makke, a particle physicst @ CERN. </p> -->
<!--       <p>My research interest focuses on investigating the origin of the universe through the study of
          elementary paticles, their formation and the theory underlying their interaction.
      </p> -->
<!--       <p> The topics of my research interest are briefly presented in &quot;<a href="index_publications.html">Research & publications</a>&quot;,
         as well as the list of publications.
         <!--A full list of publications can be found in <a href="https://inspirehep.net/literature?sort=mostrecent&size=25&page=1&q=a%20nour%20makke&ui-citation-summary=true">link 1</a>
          and <a href="https://scholar.google.com/citations?user=pb0Cw_QAAAAJ&hl=en">link 2</a>.</p-->
      <p>In  &quot;<a href="">Teaching</a>&quot; you'll find details about my teaching activities and some material.</p> -->
      <!--p>Find all contact details at &quot;<a href="contact.html">contact</a>&quot;. </p>-->
      <p style="color:#00B0DB" >Website under construction</p>
      <p>&nbsp;</p>
      <p>&nbsp;</p></td>
      <!--embed src="cernlogo.pdf" width="800px" height="2100px" /-->
      <!--img src="animatedProton.gif" alt="logo" /-->
    </div>
    <main>
    </main>
<!--     <footer>
      <div style="margin-left:50em; margin-top:15em">
        <td width="230">&copy; 2020 Nour Makke </td>
      </div>
    </footer> -->
    </div>
    </body>
</html>

